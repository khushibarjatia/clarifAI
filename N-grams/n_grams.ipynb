{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "n-grams.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spnetic-5/clarifAI/blob/main/n_grams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SURf0_7KbFrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea2c39f-8b9a-4b09-b739-6ac878c93ec1"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM9oogoecD50"
      },
      "source": [
        "\n",
        "data_path = '/content/gdrive/My Drive/merged-file.txt'\n",
        "with open(data_path, 'r') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oacjFCXhdHu4"
      },
      "source": [
        "datas = \" \".join(lines)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2g1hOOCbUw4"
      },
      "source": [
        "class MarkovChain:\n",
        "  def __init__(self):\n",
        "    self.lookup_dict = defaultdict(list)  \n",
        "  \n",
        "  def add_document(self, string):\n",
        "    preprocessed_list = self._preprocess(string)\n",
        "    pairs = self.__generate_tuple_keys(preprocessed_list)\n",
        "    for pair in pairs:\n",
        "      self.lookup_dict[pair[0]].append(pair[1])\n",
        "    pairs2 = self.__generate_2tuple_keys(preprocessed_list)\n",
        "    for pair in pairs2:\n",
        "      self.lookup_dict[tuple([pair[0], pair[1]])].append(pair[2])\n",
        "    pairs3 = self.__generate_3tuple_keys(preprocessed_list)\n",
        "    for pair in pairs3:\n",
        "      self.lookup_dict[tuple([pair[0], pair[1], pair[2]])].append(pair[3])\n",
        "  \n",
        "  def _preprocess(self, string):\n",
        "    cleaned = data.lower()\n",
        "    tokenized = word_tokenize(cleaned)\n",
        "    return tokenized\n",
        "\n",
        "  def __generate_tuple_keys(self, data):\n",
        "    if len(data) < 1:\n",
        "      return\n",
        "\n",
        "    for i in range(len(data) - 1):\n",
        "      yield [ data[i], data[i + 1] ]\n",
        "  \n",
        "  def __generate_2tuple_keys(self, data):\n",
        "    if len(data) < 2:\n",
        "      return\n",
        "\n",
        "    for i in range(len(data) - 2):\n",
        "      yield [ data[i], data[i + 1], data[i+2] ]\n",
        "    \n",
        "    \n",
        "  def __generate_3tuple_keys(self, data):\n",
        "    if len(data) < 3:\n",
        "      return\n",
        "\n",
        "    for i in range(len(data) - 3):\n",
        "      yield [ data[i], data[i + 1], data[i+2], data[i+3] ]\n",
        "    \n",
        "  def oneword(self, string):\n",
        "    return Counter(self.lookup_dict[string]).most_common()[:3]\n",
        "\n",
        "  def twowords(self, string):\n",
        "        suggest = Counter(self.lookup_dict[tuple(string)]).most_common()[:3]\n",
        "        if len(suggest)==0:\n",
        "            return self.oneword(string[-1])\n",
        "        return suggest\n",
        "\n",
        "  def threewords(self, string):\n",
        "        suggest = Counter(self.lookup_dict[tuple(string)]).most_common()[:3]\n",
        "        if len(suggest)==0:\n",
        "            return self.twowords(string[-2:])\n",
        "        return suggest\n",
        "    \n",
        "  def morewords(self, string):\n",
        "        return self.threewords(string[-3:])\n",
        "\n",
        "    \n",
        "  def generate_text(self, string):\n",
        "    if len(self.lookup_dict) > 0:\n",
        "        tokens = string.split(\" \")\n",
        "        if len(tokens)==1:\n",
        "            print(\"Next word suggestions:\", self.oneword(string))\n",
        "        elif len(tokens)==2:\n",
        "            print(\"Next word suggestions:\", self.twowords(string.split(\" \")))\n",
        "        elif len(tokens)==3:\n",
        "            print(\"Next word suggestions:\", self.threewords(string.split(\" \")))\n",
        "        elif len(tokens)>3:\n",
        "            print(\"Next word suggestions:\", self.morewords(string.split(\" \")))\n",
        "    return\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sxd5WOjbFrR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a7c6ee-c993-4904-af0d-bfa3cb490dce"
      },
      "source": [
        "my_markov = MarkovChain()\n",
        "my_markov.add_document(datas)\n",
        "my_markov.generate_text(input().lower())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaise\n",
            "Next word suggestions: [('kare', 242), ('kamaye', 134), ('banaye', 124)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}