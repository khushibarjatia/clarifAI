{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spnetic-5/clarifAI/blob/main/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVCF9Brf8W4F",
        "outputId": "9bc160d6-de99-435e-f9f6-43f93f41c5b0"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import re\n",
        "from keras.utils import to_categorical\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from google.colab import files\n",
        "nltk.download('punkt')\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "# uploaded = files.upload()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to_GdbMFPKAt"
      },
      "source": [
        "\n",
        "data_path = '/content/gdrive/My Drive/merged-file.txt'\n",
        "with open(data_path, 'r') as f:\n",
        "    lines = f.read().split('\\n')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjsgfA_hOOY6"
      },
      "source": [
        "# print(len(lines))\n",
        "data = \" \".join(lines)\n",
        "\n",
        "cleaned = data.lower()\n",
        "\n",
        "tokens = word_tokenize(cleaned)\n",
        "train_len = 3+1\n",
        "text_sequences = []\n",
        "for i in range(train_len,len(tokens)):\n",
        "    seq = tokens[i-train_len:i]\n",
        "    text_sequences.append(seq)\n",
        "sequences = {}\n",
        "count = 1\n",
        "for i in range(len(tokens)):\n",
        "    if tokens[i] not in sequences:\n",
        "        sequences[tokens[i]] = count\n",
        "        count += 1\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences) \n",
        "\n",
        "#Collecting some information   \n",
        "vocabulary_size = len(tokenizer.word_counts)+1\n",
        "\n",
        "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
        "for i in range(len(sequences)):\n",
        "    n_sequences[i] = sequences[i]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa7TUET-8oUe"
      },
      "source": [
        "train_inputs = n_sequences[:,:-1]\n",
        "train_targets = n_sequences[:,-1]\n",
        "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)\n",
        "seq_len = train_inputs.shape[1]\n",
        "train_inputs.shape\n",
        "#print(train_targets[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcYDEY3j9CJO"
      },
      "source": [
        "train_targets[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsLaH73l9PkF"
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "#LSTM Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len)) # Embedding Layer\n",
        "model.add(LSTM(50,return_sequences=True))  # 1st LSTM layer with 50 hidden layers\n",
        "model.add(Dropout(0.2))  # prevent a model from overfitting\n",
        "model.add(LSTM(50))  # 2nd LSTM layer with 50 hidden layers\n",
        "model.add(Dense(50,activation='relu'))  #Dense layer with 50 neurons and relu activation\n",
        "model.add(Dense(vocabulary_size, activation='softmax')) #Probability for each predicted word \n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_inputs,train_targets,epochs=500,verbose=1)\n",
        "model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0--eP4eS-xvw"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "input_text = input().strip().lower()\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre') #truncating removes values from sequences larger than maxlen\n",
        "print(encoded_text, pad_encoded)\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
        "  pred_word = tokenizer.index_word[i]\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWH6RTtzG-kp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}